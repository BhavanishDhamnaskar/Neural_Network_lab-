{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "_mUcW5RlEXJc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Neuron:\n",
        "    def __init__(self, input_size):\n",
        "        self.params = Parameters(input_size, 1)\n",
        "class Parameters:\n",
        "    def __init__(self, input_size, num_neurons):\n",
        "        self.weights = np.random.randn(input_size, num_neurons) * 0.1\n",
        "        self.bias = np.random.randn(1, num_neurons) * 0.1\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.weights\n",
        "\n",
        "    def get_bias(self):\n",
        "        return self.bias\n",
        "class Layer:\n",
        "    def __init__(self, input_size, num_neurons, activation_type):\n",
        "        self.neurons = [Neuron(input_size) for _ in range(num_neurons)]\n",
        "        self.activation_fn = Activation(activation_type)\n",
        "        self.input_size = input_size\n",
        "        self.num_neurons = num_neurons\n",
        "        self.activation_type = activation_type\n",
        "        self.inputs = None"
      ],
      "metadata": {
        "id": "bUKgC3UlFGCn"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LossFunction:\n",
        "    @staticmethod\n",
        "    def mse(predicted, actual):\n",
        "        return np.mean((predicted - actual) ** 2)\n",
        "\n",
        "    @staticmethod\n",
        "    def mse_derivative(predicted, actual):\n",
        "        return 2 * (predicted - actual) / actual.size\n",
        "\n",
        "    @staticmethod\n",
        "    def binary_cross_entropy(predicted, actual):\n",
        "        epsilon = 1e-15\n",
        "        predicted = np.clip(predicted, epsilon, 1 - epsilon)\n",
        "        return -np.mean(actual * np.log(predicted) + (1 - actual) * np.log(1 - predicted))\n",
        "\n",
        "    @staticmethod\n",
        "    def binary_cross_entropy_derivative(predicted, actual):\n",
        "        epsilon = 1e-15\n",
        "        predicted = np.clip(predicted, epsilon, 1 - epsilon)\n",
        "        return (predicted - actual) / (predicted * (1 - predicted))\n",
        "\n",
        "class Activation:\n",
        "    def __init__(self, type):\n",
        "        self.type = type\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        if self.type == \"relu\":\n",
        "            self.output = np.maximum(0, inputs)\n",
        "        elif self.type == \"sigmoid\":\n",
        "        # Clipping inputs to avoid overflow\n",
        "            clipped_inputs = np.clip(inputs, -709, 709)  # np.exp(709) is close to the limit\n",
        "            self.output = 1 / (1 + np.exp(-clipped_inputs))\n",
        "        elif self.type == \"linear\":\n",
        "            self.output = inputs\n",
        "        elif self.type == \"tanh\":\n",
        "            self.output = np.tanh(inputs)\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid activation function type: {self.type}\")\n",
        "        return self.output\n",
        "\n",
        "    def derivative(self):\n",
        "        if self.type == \"relu\":\n",
        "            return np.where(self.inputs > 0, 1, 0)\n",
        "        elif self.type == \"sigmoid\":\n",
        "            return self.output * (1 - self.output)\n",
        "        elif self.type == \"linear\":\n",
        "            return np.ones_like(self.inputs)\n",
        "        elif self.type == \"tanh\":\n",
        "            return 1 - np.power(self.output, 2)\n",
        "        else:\n",
        "            raise ValueError(f\"No derivative implemented for activation function type: {self.type}\")\n"
      ],
      "metadata": {
        "id": "3_aR4EP3FNgZ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class LossFunction:\n",
        "    @staticmethod\n",
        "    def mse(predicted, actual):\n",
        "        return np.mean((predicted - actual) ** 2)\n",
        "\n",
        "    @staticmethod\n",
        "    def mse_derivative(predicted, actual):\n",
        "        return 2 * (predicted - actual) / actual.size\n",
        "\n",
        "    @staticmethod\n",
        "    def binary_cross_entropy(predicted, actual):\n",
        "        epsilon = 1e-15\n",
        "        predicted = np.clip(predicted, epsilon, 1 - epsilon)\n",
        "        return -np.mean(actual * np.log(predicted) + (1 - actual) * np.log(1 - predicted))\n",
        "\n",
        "    @staticmethod\n",
        "    def binary_cross_entropy_derivative(predicted, actual):\n",
        "        epsilon = 1e-15\n",
        "        predicted = np.clip(predicted, epsilon, 1 - epsilon)\n",
        "        return (predicted - actual) / (predicted * (1 - predicted))\n",
        "\n",
        "class Activation:\n",
        "    def __init__(self, type):\n",
        "        self.type = type\n",
        "        self.forward_funcs = {\n",
        "            \"relu\": lambda x: np.maximum(0, x),\n",
        "            \"sigmoid\": lambda x: 1 / (1 + np.exp(np.clip(-x, -709, 709))),\n",
        "            \"linear\": lambda x: x,\n",
        "            \"tanh\": lambda x: np.tanh(x)\n",
        "        }\n",
        "        self.derivative_funcs = {\n",
        "            \"relu\": lambda x: np.where(x > 0, 1, 0),\n",
        "            \"sigmoid\": lambda x: x * (1 - x),\n",
        "            \"linear\": lambda _: np.ones_like(self.inputs),\n",
        "            \"tanh\": lambda x: 1 - np.power(x, 2)\n",
        "        }\n",
        "        if type not in self.forward_funcs:\n",
        "            raise ValueError(f\"Invalid activation function type: {type}\")\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        self.output = self.forward_funcs[self.type](inputs)\n",
        "        return self.output\n",
        "\n",
        "    def derivative(self):\n",
        "        if self.type not in self.derivative_funcs:\n",
        "            raise ValueError(f\"No derivative implemented for activation function type: {self.type}\")\n",
        "        return self.derivative_funcs[self.type](self.output)\n",
        "\n"
      ],
      "metadata": {
        "id": "GJupWtCcImuF"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gG45xwxzI00E"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Dropout:\n",
        "    def __init__(self, rate):\n",
        "        self.rate = rate\n",
        "        self.mask = None\n",
        "\n",
        "class Regularization:\n",
        "    @staticmethod\n",
        "    def compute(reg_type, weights, lambda_val):\n",
        "        if reg_type == \"l1\":\n",
        "            return lambda_val * np.sum(np.abs(weights)), lambda_val * np.sign(weights)\n",
        "        elif reg_type == \"l2\":\n",
        "            return (lambda_val / 2) * np.sum(np.square(weights)), lambda_val * weights\n",
        "        else:\n",
        "            raise ValueError(\"Invalid regularization type. Expected 'l1' or 'l2'.\")\n",
        "\n",
        "class GradientDescent:\n",
        "    @staticmethod\n",
        "    def update_parameters(layers, learning_rate):\n",
        "        for layer in layers:\n",
        "            if hasattr(layer, 'neurons'):\n",
        "                for neuron in layer.neurons:\n",
        "                    neuron.params.weights -= learning_rate * neuron.d_weights\n",
        "                    neuron.params.bias -= learning_rate * neuron.d_bias\n",
        "\n",
        "class ForwardPropagation:\n",
        "    @staticmethod\n",
        "    def apply_dropout(inputs, rate, training=True):\n",
        "        if training:\n",
        "            mask = np.random.binomial(1, 1 - rate, size=inputs.shape) / (1 - rate)\n",
        "            return inputs * mask\n",
        "        return inputs\n",
        "\n",
        "    @staticmethod\n",
        "    def forward_layer(layer, inputs):\n",
        "        layer.inputs = inputs\n",
        "        neuron_outputs = np.hstack([np.dot(inputs, neuron.params.get_weights()) + neuron.params.get_bias() for neuron in layer.neurons])\n",
        "        return layer.activation_fn.forward(neuron_outputs)\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(network, inputs):\n",
        "        for layer in network.layers:\n",
        "            inputs = ForwardPropagation.forward_layer(layer, inputs)\n",
        "        return inputs\n",
        "\n",
        "class BackwardPropagation:\n",
        "    @staticmethod\n",
        "    def backward_layer(layer, d_output):\n",
        "        d_output_activation = layer.activation_fn.derivative() * d_output\n",
        "        d_inputs = np.zeros((d_output_activation.shape[0], layer.input_size))\n",
        "        for i, neuron in enumerate(layer.neurons):\n",
        "            d_output_neuron = d_output_activation[:, i:i+1]\n",
        "            neuron.d_weights = np.dot(layer.inputs.T, d_output_neuron)\n",
        "            neuron.d_bias = np.sum(d_output_neuron, axis=0, keepdims=True)\n",
        "            d_inputs += np.dot(d_output_neuron, neuron.params.get_weights().T)\n",
        "        layer.inputs = d_inputs\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_gradients(network, loss_gradient):\n",
        "        for layer in reversed(network.layers):\n",
        "            BackwardPropagation.backward_layer(layer, loss_gradient)\n",
        "            loss_gradient = layer.inputs\n",
        "\n",
        "class InputNormalization:\n",
        "    def __init__(self, method='zscore'):\n",
        "        self.method = method\n",
        "        self.params = {}\n",
        "\n",
        "    def fit(self, data):\n",
        "        if self.method == 'minmax':\n",
        "            self.params['min'] = np.min(data, axis=0)\n",
        "            self.params['max'] = np.max(data, axis=0)\n",
        "        elif self.method == 'zscore':\n",
        "            self.params['mean'] = np.mean(data, axis=0)\n",
        "            self.params['std'] = np.std(data, axis=0)\n",
        "            self.params['std'][self.params['std'] == 0] = 1\n",
        "        elif self.method == 'max':\n",
        "            self.params['max'] = np.max(np.abs(data), axis=0)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown normalization method: {self.method}\")\n",
        "\n",
        "    def transform(self, data):\n",
        "        if self.method == 'minmax':\n",
        "            return (data - self.params['min']) / (self.params['max'] - self.params['min'])\n",
        "        elif self.method == 'zscore':\n",
        "            return (data - self.params['mean']) / self.params['std']\n",
        "        elif self.method == 'max':\n",
        "            return data / self.params['max']\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown normalization method: {self.method}\")\n",
        "\n",
        "    def fit_transform(self, data):\n",
        "        self.fit(data)\n",
        "        return self.transform(data)\n",
        "\n",
        "class MiniBatchGenerator:\n",
        "    def __init__(self, X, y, batch_size=32):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.batch_size = batch_size\n",
        "        self.n_samples = X.shape[0]\n",
        "        self.indices = np.arange(self.n_samples)\n",
        "        np.random.shuffle(self.indices)\n",
        "\n",
        "    def get_batches(self):\n",
        "        for start_idx in range(0, self.n_samples, self.batch_size):\n",
        "            end_idx = min(start_idx + self.batch_size, self.n_samples)\n",
        "            batch_indices = self.indices[start_idx:end_idx]\n",
        "            yield self.X[batch_indices], self.y[batch_indices]\n"
      ],
      "metadata": {
        "id": "SWrTbluhGEf8"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ChMVs5_UNj4s"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural model"
      ],
      "metadata": {
        "id": "TpbHbBwDRSsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self):\n",
        "        self.layers = []\n",
        "\n",
        "    def add_layer(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Utilize the ForwardPropagation class for the forward pass\n",
        "        return ForwardPropagation.forward(self, inputs)\n",
        "\n",
        "    def backward(self, loss_gradient):\n",
        "        # Utilize the BackwardPropagation class for computing gradients\n",
        "        BackwardPropagation.compute_gradients(self, loss_gradient)\n",
        "\n",
        "    def evaluate(self, X, y):\n",
        "        predictions = self.predict(X)\n",
        "        predictions = np.round(predictions)  # Adjust as needed for your case\n",
        "        accuracy = np.mean(predictions == y)\n",
        "        return accuracy\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        return self.forward(inputs)\n",
        "\n",
        "class NetworkTrainer:\n",
        "    def __init__(self, neural_network, learning_rate, lambda_val, batch_size):\n",
        "        self.neural_network = neural_network\n",
        "        self.learning_rate = learning_rate\n",
        "        self.lambda_val = lambda_val\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def train(self, X_train, y_train, epochs):\n",
        "        # Optionally, you could implement mini-batch training by integrating the MiniBatchGenerator here\n",
        "        for epoch in range(epochs):\n",
        "            # If using mini-batches, this loop would iterate over batches instead of the entire dataset at once\n",
        "            outputs = self.neural_network.forward(X_train)\n",
        "            loss = LossFunction.binary_cross_entropy(outputs, y_train) + self.regularization_loss()\n",
        "\n",
        "            print(f\"Epoch {epoch+1}, Loss: {loss}\")\n",
        "            loss_gradient = LossFunction.mse_derivative(outputs, y_train)\n",
        "\n",
        "            self.neural_network.backward(loss_gradient)\n",
        "            GradientDescent.update_parameters(self.neural_network.layers, self.learning_rate)  # Removed lambda_val\n",
        "\n",
        "    def regularization_loss(self):\n",
        "        # Calculate and return the regularization loss\n",
        "        reg_loss = 0\n",
        "        reg_type = \"l2\"  # or \"l1\" depending on the chosen regularization\n",
        "        for layer in self.neural_network.layers:\n",
        "            for neuron in layer.neurons:\n",
        "                reg_loss_value, _ = Regularization.compute(reg_type, neuron.params.weights, self.lambda_val)\n",
        "                reg_loss += reg_loss_value\n",
        "        return reg_loss\n",
        "\n",
        "    def evaluate(self, X, y):\n",
        "        # Utilize the NeuralNetwork's evaluate method\n",
        "        return self.neural_network.evaluate(X, y)\n"
      ],
      "metadata": {
        "id": "ByI2KbNZGi7V"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dataset = pd.read_csv(\"/content/train_x.csv\")\n",
        "labels = pd.read_csv(\"/content/train_label.csv\")\n",
        "\n",
        "# Analyzing the shapes of the dataset and labels\n",
        "dataset_shape = dataset.shape\n",
        "labels_shape = labels.shape\n",
        "\n",
        "dataset_shape, labels_shape\n",
        "\n",
        "# Splitting the dataset into training, validation, and test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(dataset, labels, test_size=0.2, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Checking the shapes of the splits\n",
        "(X_train.shape, y_train.shape), (X_val.shape, y_val.shape), (X_test.shape, y_test.shape)\n",
        "\n",
        "input_normalizer = InputNormalization(method='zscore')\n",
        "# Convert the pandas dataframes to numpy arrays for processing with the neural network\n",
        "X_train_np = input_normalizer.fit_transform(X_train.to_numpy())\n",
        "X_val_np = input_normalizer.transform(X_val.to_numpy())\n",
        "X_test_np = input_normalizer.transform(X_test.to_numpy())\n",
        "\n",
        "y_train_np = y_train.to_numpy()\n",
        "y_val_np = y_val.to_numpy()\n",
        "y_test_np = y_test.to_numpy()\n",
        "\n",
        "input_size = 784\n",
        "output_size = 10\n",
        "epochs = 100\n",
        "learning_rate = 0.1\n",
        "lambda_val = 0.001\n",
        "batch_size = 32\n",
        "\n",
        "nn = NeuralNetwork()\n",
        "nn.add_layer(Layer(input_size=input_size, num_neurons=10, activation_type='relu'))\n",
        "nn.add_layer(Layer(input_size=10, num_neurons=8, activation_type='relu'))\n",
        "nn.add_layer(Layer(input_size=8, num_neurons=8, activation_type='relu'))  # Second layer with 8 neurons and ReLU\n",
        "nn.add_layer(Layer(input_size=8, num_neurons=4, activation_type='relu'))\n",
        "nn.add_layer(Layer(input_size=4, num_neurons=1, activation_type='sigmoid'))\n",
        "\n",
        "trainer = NetworkTrainer(nn, learning_rate, lambda_val, batch_size)\n",
        "\n",
        "# Training the neural network using the trainer\n",
        "print(\"Training the Neural Network...\")\n",
        "trainer.train(X_train_np, y_train_np, epochs)\n",
        "# Evaluating the neural network\n",
        "print(\"Evaluating on Validation Data...\")\n",
        "val_accuracy = nn.evaluate(X_val_np, y_val_np)\n",
        "print(f\"Validation Accuracy: {val_accuracy}\")\n",
        "\n",
        "print(\"Evaluating on Test Data...\")\n",
        "test_accuracy = nn.evaluate(X_test_np, y_test_np)\n",
        "print(f\"Test Accuracy: {test_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkDRjw9RRYcQ",
        "outputId": "10abae45-d46d-4d67-9c30-76e902c64ce7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the Neural Network...\n",
            "Epoch 1, Loss: 0.7110398254833417\n",
            "Epoch 2, Loss: 0.7102794571204073\n",
            "Epoch 3, Loss: 0.7095210856842761\n",
            "Epoch 4, Loss: 0.7087647068086229\n",
            "Epoch 5, Loss: 0.7080103161227821\n",
            "Epoch 6, Loss: 0.707257909251894\n",
            "Epoch 7, Loss: 0.7065074818170526\n",
            "Epoch 8, Loss: 0.705759051400944\n",
            "Epoch 9, Loss: 0.7050126561638832\n",
            "Epoch 10, Loss: 0.704268273520327\n",
            "Epoch 11, Loss: 0.7035259186143801\n",
            "Epoch 12, Loss: 0.7027855200126766\n",
            "Epoch 13, Loss: 0.702047073327456\n",
            "Epoch 14, Loss: 0.7013105741755201\n",
            "Epoch 15, Loss: 0.7005760318987774\n",
            "Epoch 16, Loss: 0.6998434703564798\n",
            "Epoch 17, Loss: 0.6991129588689761\n",
            "Epoch 18, Loss: 0.6983845182343641\n",
            "Epoch 19, Loss: 0.697658146977061\n",
            "Epoch 20, Loss: 0.6969338375287688\n",
            "Epoch 21, Loss: 0.6962115011822742\n",
            "Epoch 22, Loss: 0.6954910988520582\n",
            "Epoch 23, Loss: 0.6947727167891832\n",
            "Epoch 24, Loss: 0.6940563697872331\n",
            "Epoch 25, Loss: 0.693342025275244\n",
            "Epoch 26, Loss: 0.6926297278302973\n",
            "Epoch 27, Loss: 0.6919194783531398\n",
            "Epoch 28, Loss: 0.691211277445414\n",
            "Epoch 29, Loss: 0.6905050942949732\n",
            "Epoch 30, Loss: 0.6898008755545532\n",
            "Epoch 31, Loss: 0.6890986366100511\n",
            "Epoch 32, Loss: 0.6883984213411969\n",
            "Epoch 33, Loss: 0.6877001785962175\n",
            "Epoch 34, Loss: 0.6870038934404098\n",
            "Epoch 35, Loss: 0.6863096126697272\n",
            "Epoch 36, Loss: 0.6856174032682045\n",
            "Epoch 37, Loss: 0.6849272344404916\n",
            "Epoch 38, Loss: 0.6842390827966829\n",
            "Epoch 39, Loss: 0.6835528756784411\n",
            "Epoch 40, Loss: 0.6828685207054058\n",
            "Epoch 41, Loss: 0.6821860863092283\n",
            "Epoch 42, Loss: 0.6815055971012915\n",
            "Epoch 43, Loss: 0.6808270795729314\n",
            "Epoch 44, Loss: 0.6801505372187227\n",
            "Epoch 45, Loss: 0.6794759135591003\n",
            "Epoch 46, Loss: 0.6788031976726618\n",
            "Epoch 47, Loss: 0.6781324774923115\n",
            "Epoch 48, Loss: 0.6774638309662024\n",
            "Epoch 49, Loss: 0.6767971578129504\n",
            "Epoch 50, Loss: 0.6761324664729775\n",
            "Epoch 51, Loss: 0.6754697573068174\n",
            "Epoch 52, Loss: 0.6748090550488597\n",
            "Epoch 53, Loss: 0.6741502616972858\n",
            "Epoch 54, Loss: 0.6734933130427467\n",
            "Epoch 55, Loss: 0.6728382692745762\n",
            "Epoch 56, Loss: 0.6721851372964953\n",
            "Epoch 57, Loss: 0.6715338806573998\n",
            "Epoch 58, Loss: 0.6708844831077555\n",
            "Epoch 59, Loss: 0.6702370162394314\n",
            "Epoch 60, Loss: 0.6695914030622103\n",
            "Epoch 61, Loss: 0.6689476604336265\n",
            "Epoch 62, Loss: 0.6683057158913743\n",
            "Epoch 63, Loss: 0.6676655888252662\n",
            "Epoch 64, Loss: 0.6670273207224195\n",
            "Epoch 65, Loss: 0.6663908385344752\n",
            "Epoch 66, Loss: 0.6657561805570985\n",
            "Epoch 67, Loss: 0.6651233662714455\n",
            "Epoch 68, Loss: 0.6644923897686992\n",
            "Epoch 69, Loss: 0.6638632840917574\n",
            "Epoch 70, Loss: 0.663235984308927\n",
            "Epoch 71, Loss: 0.6626104610071137\n",
            "Epoch 72, Loss: 0.6619867276605793\n",
            "Epoch 73, Loss: 0.6613648189408367\n",
            "Epoch 74, Loss: 0.6607446653180371\n",
            "Epoch 75, Loss: 0.6601263424318236\n",
            "Epoch 76, Loss: 0.6595097412571708\n",
            "Epoch 77, Loss: 0.6588949645204063\n",
            "Epoch 78, Loss: 0.6582819465410652\n",
            "Epoch 79, Loss: 0.6576706338526452\n",
            "Epoch 80, Loss: 0.6570610786518328\n",
            "Epoch 81, Loss: 0.6564532645475052\n",
            "Epoch 82, Loss: 0.6558471433625511\n",
            "Epoch 83, Loss: 0.655242659820173\n",
            "Epoch 84, Loss: 0.6546398286787932\n",
            "Epoch 85, Loss: 0.6540386870902608\n",
            "Epoch 86, Loss: 0.6534391725312384\n",
            "Epoch 87, Loss: 0.6528413483792511\n",
            "Epoch 88, Loss: 0.6522452216978114\n",
            "Epoch 89, Loss: 0.6516507521696286\n",
            "Epoch 90, Loss: 0.6510579599110701\n",
            "Epoch 91, Loss: 0.6504668120574133\n",
            "Epoch 92, Loss: 0.6498772678355157\n",
            "Epoch 93, Loss: 0.6492893179760474\n",
            "Epoch 94, Loss: 0.6487029893799127\n",
            "Epoch 95, Loss: 0.6481182554866983\n",
            "Epoch 96, Loss: 0.6475351179621195\n",
            "Epoch 97, Loss: 0.646953569703455\n",
            "Epoch 98, Loss: 0.6463736306060187\n",
            "Epoch 99, Loss: 0.6457952656345273\n",
            "Epoch 100, Loss: 0.645218480299632\n",
            "Evaluating on Validation Data...\n",
            "Validation Accuracy: 0.9\n",
            "Evaluating on Test Data...\n",
            "Test Accuracy: 0.9\n"
          ]
        }
      ]
    }
  ]
}